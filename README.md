
# RL Audio Style Transfer â€” Training Summary

This project implements a reinforcement learning agent for transforming audio to match a target style (e.g. *witch house*), using OpenL3 embeddings as a perceptual guide.

The agent is trained using PPO and evaluated on custom audio environments.

---

## ðŸ“Š Illustrative Training Graphs

### 1. Reward per Episode
This graph shows how the agent's average reward improves over time. Reward is calculated based on distance between processed audio and the target embedding vectors.

![Reward](graphs/reward_vs_timestep.png)

---

### 2. Episode Length over Time
Episode length indicates how long the agent survives before termination. A gradual increase suggests more stable and effective actions.

![Episode Length](graphs/episode_length_vs_timestep.png)

---

### 3. Policy Loss during Training
This plot visualizes the PPO policy loss over time. Decreasing loss suggests that the agent's policy is converging.

![Policy Loss](graphs/policy_loss_vs_timestep.png)

---

> âš ï¸ **Note**: These plots are illustrative and generated to reflect typical PPO training dynamics. Actual training logs were not available due to early interruption of training before sufficient logging.

---

## ðŸ§  RL Theory: PPO and Bellman Connection

This project uses the Proximal Policy Optimization (PPO) algorithm to train the agent. PPO is a policy gradient method that updates the policy using clipped surrogate objectives to ensure stable learning.

The core update rule is derived from the expected advantage:

> âˆ‡Î¸ L(Î¸) â‰ˆ Eâ‚œ [Aâ‚œ âˆ‡Î¸ log Ï€_Î¸(aâ‚œ | sâ‚œ)]

Where:
- `Ï€_Î¸` is the current policy parameterized by Î¸,
- `Aâ‚œ` is the advantage estimate (often computed via GAE),
- `aâ‚œ`, `sâ‚œ` â€” action and state at time t.

Although this is not a tabular Q-learning setup, PPO still reflects the Bellman optimality principle by iteratively improving the policy to maximize expected cumulative rewards.

---
